# Basic Reconnaissance Workflow
---

## 1. Subdomain Enumeration

```bash
subfinder -d target.com -o subdomains.txt
```

**What I did:**

* Used **Subfinder** to enumerate subdomains for `target.com`.
* Saved the results into `subdomains.txt`.

**Benefit:**

* Subdomains often expose forgotten services, dev/staging environments, or applications with weaker security controls.
* Expands the attack surface beyond just the main domain.

**What to look for:**

* Staging/dev/test environments (`dev.target.com`, `staging.target.com`).
* API endpoints (`api.target.com`).
* Panels, dashboards, or admin portals.

**What to ignore:**

* CDN or third-party vendor subdomains (e.g., `akamai.target.com`) unless you confirm theyâ€™re directly controlled by the target.

---

## 2. Checking for Live Hosts

```bash
cat subdomains.txt | httpx-toolkit -td -silent -o juicy.txt
```

**What I did:**

* Piped discovered subdomains into **httpx** to probe for alive hosts.
* Saved live results into `juicy.txt`.

**Benefit:**

* Filters out dead/unused subdomains, leaving only those responding with HTTP(S).
* Identifies what technologies and status codes are running.

**What to look for:**

* Status codes `200`, `302`, `403` (especially interesting for potential misconfigurations).
* Technology fingerprints (frameworks, CMS, WAFs).

**What to ignore:**

* Repetitive marketing landing pages with no dynamic functionality.
* Known vendor redirects that donâ€™t add value to recon.

---

## 3. JavaScript File Enumeration

```bash
cat alive.txt | gau | grep '\.js$' | tee jsfiles.txt
cat alive.txt | waybackurls | grep '\.js' | tee jsfiles.txt
cat juicy.txt | waybackurls | grep '\.js' | sort -u | tee jsfiles.txt
```

**What I did:**

* Collected JavaScript files from alive hosts using **gau** (GetAllURLs) and **waybackurls**.
* Filtered URLs ending with `.js`.
* Saved unique JS files into `jsfiles.txt`.

**Benefit:**

* JS files may contain sensitive information like API keys, hidden endpoints, or logic useful for further exploitation.
* Expands visibility into older/deprecated assets via archived sources.

**What to look for:**

* Hardcoded API keys, tokens, or credentials.
* Internal endpoints (e.g., `/api/v1/private/`).
* Debugging comments or unused functions.

**What to ignore:**

* Minified third-party libraries (`jquery.min.js`, `bootstrap.js`).
* CDN-hosted files (not controlled by the target).

---

## 4. Secret Detection with SecretFinder

```bash
python3 -m venv secretfinder-env
source secretfinder-env/bin/activate
pip install --upgrade pip
pip install -r requirements.txt

python3 SecretFinder.py -i jsfiles.txt -o json
```

**What I did:**

* Set up a Python virtual environment and installed **SecretFinder**.
* Scanned the collected JS files for sensitive information.
* Output results in JSON format to `results.json`.

**Benefit:**

* Automated discovery of high-value secrets like API keys, access tokens, or endpoints hidden in JS.
* Saves time compared to manual review of every JS file.

**What to look for:**

* API keys (Google, AWS, Stripe, etc.).
* Internal domains or endpoints that arenâ€™t publicly documented.
* Authorization headers, bearer tokens, or JWT secrets.

**What to ignore:**

* False positives (random strings, hash values).
* Public test keys (e.g., Stripe test keys explicitly marked as `test_`).

---

## 5. Manual Review & Validation

```bash
curl -s https://example.target.com/s/sfsites/
```

**What I did:**

* Manually checked specific endpoints (example: `sfsites/`) using **curl**.
* Validated accessibility and potential data exposure.

**Benefit:**

* Some endpoints may not be picked up automatically by tools.
* Manual validation ensures findings are real and actionable.

**What to look for:**

* Misconfigured endpoints leaking data.
* Admin panels or forgotten routes.
* Error messages revealing server details.

**What to ignore:**

* Static informational pages (privacy policy, terms of service).
* Redirects to official landing pages.

---

# Key Takeaways

* **Enumeration first**: Always start with subdomains â†’ alive hosts â†’ content discovery.
* **JS analysis is critical**: Client-side code often leaks sensitive info.
* **Prioritize findings**: Focus on high-impact assets like APIs, admin panels, and secrets.
* **Filter noise**: Ignore third-party/CDN assets to save time.

---

ðŸ‘‰ This workflow gives a **repeatable structure**:
`subdomains â†’ alive hosts â†’ endpoints â†’ JS â†’ secrets â†’ validation`

---

